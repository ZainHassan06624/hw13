{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 13 <br> Zain Hassan <br> zh06624@st.habib.edu.pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import numpy\n",
    "import tweepy as tw\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Select any article on the website of your interest and perform Name Entity Relation (NER) analysis on it. Show all the necessary output and visualization and explain your findings. Also mention if you see any shortcoming or error in the algorithm output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_string(url):\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "        return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_bb = url_to_string('https://www.nba.com/news/kings-hope-for-crowd-to-carry-them-in-game-7-vs-warriors')\n",
    "article = nlp(ny_bb)\n",
    "len(article.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [x for x in article.sents]\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(str(sentences)), jupyter=True, style='ent')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can the whole article is tagged with name entity recognization and it had marked different entities such as person, location, organization, events and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [x.label_ for x in article.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see the frequency of each entity found from our article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [x.text for x in article.ents]\n",
    "Counter(items).most_common(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We found out the 5 most commom entities in the article"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors in the algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TVNewsHomeTop StoriesAwardsFeaturesDraftHistoryLearn -> PERSON - This is not a person<br>\n",
    "PassCustomer Support -> PERSON - This is not a person<br>\n",
    "Game 5 -> LAW - This ia an event\n",
    "Butler -> ORG - This is a person"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: Select any research paper of your interest and perform word frequency and keyword extraction analysis. Use at least 3 different techniques/libraries. Show all the necessary output and visualization and explain your findings. Compare different techniques and mention if you see any shortcoming or error in the algorithm output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tika import parser\n",
    "raw= parser.from_file('research.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_words = nltk.tokenize.word_tokenize(raw['content'])\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend([\"T\", \"J\", \"D\", \"S\", \"M\", \"H\", \"et al\", \"et\", \"pp\", \"a.\", \"m.\", \"d.\", \"j.\", \"s.\", \"r.\", \"c.\", \"t.\"])\n",
    "no_stopwords = [word for word in tokenize_words if word.lower() not in stopwords]\n",
    "\n",
    "# remove punctuation, count raw words\n",
    "nonPunct = re.compile('.*[A-Za-z].*')\n",
    "clean_words = [w for w in no_stopwords if nonPunct.match(w)]\n",
    "    \n",
    "filtered_word_freq = nltk.FreqDist(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_freq.plot(30,title='Frequency distribution for 30 most common tokens in our text collection (excluding stopwords and punctuation)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='07558_CenturyGothic.ttf'\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\", font_path=path).generate(raw['content'])\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"T\", \"J\", \"D\", \"S\", \"M\", \"H\", \"et al\", \"et\", \"al\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"white\").generate(raw['content'])\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(raw['content'])\n",
    "r.get_ranked_phrases_with_scores()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc2 = nlp(raw['content'])\n",
    "print(doc2.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "language = \"en\"\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 20\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(raw['content'])\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing required modules\n",
    "# from PyPDF2 import PdfReader\n",
    "\n",
    "# # creating a pdf reader object\n",
    "# reader = PdfReader('research.pdf')\n",
    "\n",
    "# # printing number of pages in pdf file\n",
    "# print(len(reader.pages))\n",
    "\n",
    "# # getting a specific page from the pdf file\n",
    "# page = reader.pages[0]\n",
    "\n",
    "# # extracting text from page\n",
    "# raw = page.extract_text()\n",
    "# # print(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: 3. Choose any keyword of your interest and pull at least 1000 (tweets) from Twitter.\n",
    "#### a. Clean the tweets and store them in csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'eR1zb853CsroHzovzLiJNPBK8'\n",
    "consumer_key_secret = 'SMTGv6dLoAj81HAJ1q0I8DQ19pTTscdAINCXynLHdxb6tUDVjA'\n",
    "access_token = '107436862-6ywCOzDd5bSHLZd3BWJ3buacPsL4OpWmhV5Lu9Uk'\n",
    "access_token_secret = 'JAfDMXeg0OYz9ax8dELqvSlfX1sduHDVHnslVWwJjiJc2'\n",
    "auth = tw.OAuth2BearerHandler('AAAAAAAAAAAAAAAAAAAAAM6CnAEAAAAAg3jcrw7guJg1MWNZ6avdp1hP2sk%3DMLBlokDJlF6H7yNGQ5OrS5WWT0Yqb16rvADj9pvYXLhQq2gwhz')\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"#pti -filter:retweets\"\n",
    "\n",
    "tweets = tw.Cursor(api.search_tweets,\n",
    "                   q=search_term + 'since:2023-04-30',\n",
    "                   lang=\"en\").items(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_tweets = [t for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part-3: Creating Dataframe of Tweets\n",
    "\n",
    "#Cleaning searched tweets and converting into Dataframe\n",
    "my_list_of_dicts = []\n",
    "for each_json_tweet in searched_tweets:\n",
    "    my_list_of_dicts.append(each_json_tweet._json)\n",
    "    \n",
    "with open('tweet_pti.txt', 'w') as file:\n",
    "        file.write(json.dumps(my_list_of_dicts, indent=4))\n",
    "        \n",
    "my_demo_list = []\n",
    "with open('tweet_pti.txt', encoding='utf-8') as json_file:  \n",
    "    all_data = json.load(json_file)\n",
    "    for each_dictionary in all_data:\n",
    "        tweet_id = each_dictionary['id']\n",
    "        text = each_dictionary['text']\n",
    "        favorite_count = each_dictionary['favorite_count']\n",
    "        retweet_count = each_dictionary['retweet_count']\n",
    "        created_at = each_dictionary['created_at']\n",
    "        my_demo_list.append({'tweet_id': str(tweet_id),\n",
    "                             'text': str(text),\n",
    "                             'favorite_count': int(favorite_count),\n",
    "                             'retweet_count': int(retweet_count),\n",
    "                             'created_at': created_at,\n",
    "                            })\n",
    "        \n",
    "        tweet_dataset = pd.DataFrame(my_demo_list, columns = \n",
    "                                  ['tweet_id', 'text', \n",
    "                                   'favorite_count', 'retweet_count', \n",
    "                                   'created_at'])\n",
    "    \n",
    "#Writing tweet dataset ti csv file for future reference\n",
    "tweet_dataset.to_csv('tweet_pti.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    #Replace URLs found in a text string with nothing\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset['text'] = np.vectorize(remove_url)(tweet_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset['text'] = np.vectorize(remove_pattern)(tweet_dataset['text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset.to_csv('tweet_pti_clean.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Perform Sentiment Analysis on the tweet and show total positive and negative sentiment counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_sentiment(txt):\n",
    "    analysis = TextBlob(txt)\n",
    "    if analysis.sentiment[0]>=0:\n",
    "       res = 'positive'\n",
    "    else:\n",
    "       res= 'negative'\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset['sentiment'] = np.vectorize(tweet_sentiment)(tweet_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset['sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are a total of 1000 tweets out of which 898 are positive and 102 are negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ' '.join(tweet_dataset.text)\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"T\", \"J\", \"D\", \"S\", \"M\", \"H\", \"et al\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"white\").generate(corpus)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Display one Positive and one Negative sentiment and discuss if the algorithm is accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_row = tweet_dataset[tweet_dataset['sentiment'] == 'positive'].sample(n=1)\n",
    "negative_row = tweet_dataset[tweet_dataset['sentiment'] == 'negative'].sample(n=1)\n",
    "\n",
    "\n",
    "print(\"Positive Sentiment:\")\n",
    "print(positive_row)\n",
    "\n",
    "print(\"\\nNegative Sentiment:\")\n",
    "print(negative_row)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Of Algorithm\n",
    "#### It'll give different values everytime, but the value i've gotten for positive is 172, and 5 for negative, these are indexes. 172 is predicted to positive where as in reality it should have been negative  and number 5 is correct to be negative as it is a negative comment\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Display the tweet with highest retweets and show its sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset[tweet_dataset['retweet_count']== tweet_dataset['retweet_count'].max()].sample(n=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Make a timeline visualization plot showing counts of positive and negative sentiments over the period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = tweet_dataset.groupby(['created_at', 'sentiment']).size().reset_index(name='count')\n",
    "\n",
    "# Reshape the data for visualization\n",
    "reshaped = grouped.pivot(index='created_at', columns='sentiment', values='count')\n",
    "\n",
    "# Plot the timeline visualization\n",
    "ax = reshaped.plot(kind='area', stacked=True)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Sentiment Count')\n",
    "plt.title('Timeline Visualization of Sentiments')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
